{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/tamobee/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tamobee/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Initial imports\n",
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "from pathlib import Path\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-01-15 04:24:47</th>\n",
       "      <td>@SuperclusterHQ @w00ki33 Fallout New Texas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-15 03:23:28</th>\n",
       "      <td>@Breedlove22 @benmezrich Only Chuck Norris can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-15 03:18:10</th>\n",
       "      <td>@Cerberu21014829 @Breedlove22 @benmezrich Good...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-15 02:12:06</th>\n",
       "      <td>@Breedlove22 @benmezrich The thing we call mon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-15 01:44:22</th>\n",
       "      <td>Monty Python is amazing  https://t.co/UJq94IWT88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 tweet\n",
       "date                                                                  \n",
       "2021-01-15 04:24:47         @SuperclusterHQ @w00ki33 Fallout New Texas\n",
       "2021-01-15 03:23:28  @Breedlove22 @benmezrich Only Chuck Norris can...\n",
       "2021-01-15 03:18:10  @Cerberu21014829 @Breedlove22 @benmezrich Good...\n",
       "2021-01-15 02:12:06  @Breedlove22 @benmezrich The thing we call mon...\n",
       "2021-01-15 01:44:22   Monty Python is amazing  https://t.co/UJq94IWT88"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = Path(\"Resources/elon_tweets.csv\")\n",
    "tweets_data = pd.read_csv(file_path, index_col=\"date\")\n",
    "tweets_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from string import punctuation\n",
    "import string\n",
    "import re\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_addon = {'char', 'has', 'thing'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    \"\"\"Tokenizes text.\"\"\"\n",
    "    \n",
    "    regex = re.compile(\"[^a-zA-Z ]\")\n",
    "    re_clean = regex.sub('', text)\n",
    "    words = word_tokenize(re_clean)\n",
    "    sw = set(stopwords.words('english'))\n",
    "    lem = [lemmatizer.lemmatize(word) for word in words]\n",
    "    tokens = [word for word in lem if word.lower() not in sw.union(sw_addon)]\n",
    "                          \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-01-15 04:24:47</th>\n",
       "      <td>[SuperclusterHQ, wki, Fallout, New, Texas]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-15 03:23:28</th>\n",
       "      <td>[Breedlove, benmezrich, Chuck, Norris, divide,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-15 03:18:10</th>\n",
       "      <td>[Cerberu, Breedlove, benmezrich, Good, point]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-15 02:12:06</th>\n",
       "      <td>[Breedlove, benmezrich, call, money, informati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-15 01:44:22</th>\n",
       "      <td>[Monty, Python, amazing, httpstcoUJqIWT]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 tweet\n",
       "date                                                                  \n",
       "2021-01-15 04:24:47         [SuperclusterHQ, wki, Fallout, New, Texas]\n",
       "2021-01-15 03:23:28  [Breedlove, benmezrich, Chuck, Norris, divide,...\n",
       "2021-01-15 03:18:10      [Cerberu, Breedlove, benmezrich, Good, point]\n",
       "2021-01-15 02:12:06  [Breedlove, benmezrich, call, money, informati...\n",
       "2021-01-15 01:44:22           [Monty, Python, amazing, httpstcoUJqIWT]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_data[\"tweet\"] = tweets_data['tweet'].apply(tokenizer)\n",
    "tweets_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seriesToList(s):\n",
    "    \n",
    "    lst = []      \n",
    "    # traverse in the lists   \n",
    "    for ele in s:  \n",
    "        lst += ele     \n",
    "    # return list   \n",
    "    return lst\n",
    "\n",
    "tweets = seriesToList(tweets_data[\"tweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('amp',), 1824),\n",
       " (('Tesla',), 1684),\n",
       " (('SpaceX',), 646),\n",
       " (('Erdayastronaut',), 575),\n",
       " (('car',), 535),\n",
       " (('wa',), 475),\n",
       " (('Yes',), 471),\n",
       " (('flcnhvy',), 433),\n",
       " (('Model',), 393),\n",
       " (('year',), 390),\n",
       " (('like',), 386),\n",
       " (('time',), 329),\n",
       " (('would',), 312),\n",
       " (('make',), 310),\n",
       " (('ha',), 309),\n",
       " (('good',), 307),\n",
       " (('much',), 286),\n",
       " (('need',), 276),\n",
       " (('one',), 255),\n",
       " (('PPathole',), 250),\n",
       " (('great',), 250),\n",
       " (('rocket',), 249),\n",
       " (('work',), 241),\n",
       " (('people',), 239),\n",
       " (('get',), 237),\n",
       " (('high',), 228),\n",
       " (('next',), 227),\n",
       " (('soon',), 224),\n",
       " (('Yeah',), 220),\n",
       " (('engine',), 206),\n",
       " (('Teslarati',), 206),\n",
       " (('week',), 206),\n",
       " (('right',), 205),\n",
       " (('teslaownersSV',), 203),\n",
       " (('way',), 201),\n",
       " (('day',), 199),\n",
       " (('launch',), 197),\n",
       " (('better',), 192),\n",
       " (('production',), 190),\n",
       " (('month',), 190),\n",
       " (('go',), 188),\n",
       " (('team',), 187),\n",
       " (('Falcon',), 178),\n",
       " (('test',), 177),\n",
       " (('many',), 171),\n",
       " (('Good',), 170),\n",
       " (('Exactly',), 168),\n",
       " (('Thanks',), 168),\n",
       " (('dont',), 167),\n",
       " (('Mars',), 166)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = ngrams(tweets, n=1)\n",
    "tweets_dict = dict(Counter(bigrams).most_common(50))\n",
    "tweets_bigrams = tweets_dict.items()\n",
    "list(tweets_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_count(tokens, N=15):\n",
    "    \"\"\"Returns the top N tokens from the frequency count\"\"\"\n",
    "    return Counter(tokens).most_common(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('amp', 1824),\n",
       " ('Tesla', 1684),\n",
       " ('SpaceX', 646),\n",
       " ('Erdayastronaut', 575),\n",
       " ('car', 535),\n",
       " ('wa', 475),\n",
       " ('Yes', 471),\n",
       " ('flcnhvy', 433),\n",
       " ('Model', 393),\n",
       " ('year', 390),\n",
       " ('like', 386),\n",
       " ('time', 329),\n",
       " ('would', 312),\n",
       " ('make', 310),\n",
       " ('ha', 309)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_count(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "nlpenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
